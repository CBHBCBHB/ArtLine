{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM/9bk/VU+WBRmhWO4g28lJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijishmadhavan/ArtLine/blob/main/ControlNet_%2BArtLine_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **From https://github.com/vijishmadhavan/ArtLine**"
      ],
      "metadata": {
        "id": "3iyZqhSSa2VC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Installing ArtLine Dependencies\n",
        "\n",
        "!wget https://www.dropbox.com/s/m9ma320rapswk2z/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "fHs4spFFnuqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##  Clone ControlNet\n",
        "#@markdown Clone ControlNet from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.\n",
        "\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "root_dir = \"/content\"\n",
        "%store root_dir\n",
        "repo_dir = str(root_dir)+\"/ControlNet\"\n",
        "%store repo_dir\n",
        "models_dir = str(root_dir)+\"/ControlNet/models\"\n",
        "%store models_dir\n",
        "\n",
        "\n",
        "branch = \"\" #@param {type: \"string\"}\n",
        "repo_url = \"https://github.com/lllyasviel/ControlNet\"\n",
        "\n",
        "def clone_repo():\n",
        "  if os.path.isdir(repo_dir):\n",
        "    print(\"The repository folder already exists, will do a !git pull instead\\n\")\n",
        "    %cd {repo_dir}\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "  else:\n",
        "    %cd {root_dir}\n",
        "    !git clone {repo_url} {repo_dir}\n",
        "\n",
        "if not os.path.isdir(repo_dir):\n",
        "  clone_repo()\n",
        "\n",
        "%cd {root_dir}\n",
        "os.makedirs(repo_dir, exist_ok=True)\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "if branch:\n",
        "  %cd {repo_dir}\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "975XamgmiEjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Installing Dependencies\n",
        "#@markdown This will install required Python packages\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "def install_dependencies():\n",
        "    if install_xformers:\n",
        "        !pip install gradio -q #gradio isn't required to be mentioned on requirements for apps on HF space\n",
        "        !sudo apt-get install aria2\n",
        "        !pip install timm==0.6.12\n",
        "        !pip install opencv-contrib-python==4.3.0.36\n",
        "        !pip install pytorch-lightning==1.9.1\n",
        "        !pip install einops==0.3.0\n",
        "        !pip install open_clip_torch==2.0.2\n",
        "        !pip install omegaconf==2.1.1\n",
        "        !pip install transformers==4.19.2\n",
        "        !pip install xformers --no-cache-dir\n",
        "        !pip install --pre -U triton\n",
        "        !sudo apt-get install aria2\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_canny2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_annotator.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_depth2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_fake_scribble2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_hed2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_hough2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_normal2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_pose2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_scribble2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_seg2image.py\n",
        "        !sed -i \"s@block.launch(server_name='0.0.0.0')@block.launch(debug=True, share=True)@\" /content/ControlNet/gradio_scribble2image_interactive.py\n",
        "        !sed -i \"s@save_memory = False@save_memory = True@\" /content/ControlNet/config.py\n",
        "\n",
        "install_dependencies()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pHVJq40mW55g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Download ControlNet Model \n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installModels = []\n",
        "installv2Models = []\n",
        "\n",
        "#@markdown ### Available Model\n",
        "#@markdown You must download the model before launching the Gradio interface\n",
        "#@markdown Select one of available model to download:\n",
        "\n",
        "\n",
        "modelUrl = [\"\", \\\n",
        "            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth\", \\\n",
        "            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth\", \\\n",
        "            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_hed.pth\", \\\n",
        "            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth\", \\\n",
        "            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_normal.pth\", \\\n",
        "            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth\", \\\n",
        "            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth\", \\\n",
        "            \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_seg.pth\"]\n",
        "modelList = [\"\", \\\n",
        "             \"control_sd15_canny\", \\\n",
        "             \"control_sd15_depth\", \\\n",
        "             \"control_sd15_hed\", \\\n",
        "             \"control_sd15_mlsd\", \\\n",
        "             \"control_sd15_normal\", \\\n",
        "             \"control_sd15_openpose\", \\\n",
        "             \"control_sd15_scribble\", \\\n",
        "             \"control_sd15_seg\" ]\n",
        "modelName = \"control_sd15_canny\" #@param [\"control_sd15_canny\", \"control_sd15_depth\", \"control_sd15_hed\", \"control_sd15_mlsd\", \"control_sd15_normal\", \"control_sd15_openpose\", \"control_sd15_scribble\", \"control_sd15_seg\"]\n",
        "\n",
        "\n",
        "if modelName != \"\":\n",
        "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  ext = \"pth\" if url.endswith(\".pth\") else \"pt\"\n",
        "\n",
        "  hf_token = '' \n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {repo_dir}/models -o {checkpoint_name}.{ext} \"{url}\"\n",
        "\n",
        "def install_checkpoint():\n",
        "  for model in installModels:\n",
        "    install(model[0], model[1])\n",
        "\n",
        "install_checkpoint()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BgOOJrzLW7RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Download Additional model\n",
        "#@markdown You must download the model before launching the Gradio interface\n",
        "\n",
        "!aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/ControlNet/annotator/ckpts -o body_pose_model.pth \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth\"\n",
        "!aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/ControlNet/annotator/ckpts -o dpt_hybrid-midas-501f0c75.pt \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt\"\n",
        "!aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/ControlNet/annotator/ckpts -o hand_pose_model.pth \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth\"\n",
        "!aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/ControlNet/annotator/ckpts -o mlsd_large_512_fp32.pth \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth\"\n",
        "!aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/ControlNet/annotator/ckpts -o mlsd_tiny_512_fp32.ptht \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.pth\"\n",
        "!aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/ControlNet/annotator/ckpts -o network-bsds500.pth \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth\"\n",
        "!aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/ControlNet/annotator/ckpts -o upernet_global_small.pth \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "e2EwdENMXACQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Run SD\n",
        "\n",
        "%cd /content/ControlNet\n",
        "from share import *\n",
        "\n",
        "import config\n",
        "\n",
        "import cv2\n",
        "import einops\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "from pytorch_lightning import seed_everything\n",
        "from annotator.util import resize_image, HWC3\n",
        "from annotator.canny import CannyDetector\n",
        "from cldm.model import create_model, load_state_dict\n",
        "from cldm.ddim_hacked import DDIMSampler\n",
        "\n",
        "\n",
        "\n",
        "apply_canny = CannyDetector()\n",
        "\n",
        "model = create_model('./models/cldm_v15.yaml').cpu()\n",
        "model.load_state_dict(load_state_dict('./models/control_sd15_canny.pth', location='cuda'))\n",
        "model = model.cuda()\n",
        "ddim_sampler = DDIMSampler(model)\n",
        "\n",
        "\n",
        "def process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold):\n",
        "    with torch.no_grad():\n",
        "        img = resize_image(HWC3(input_image), image_resolution)\n",
        "        H, W, C = img.shape\n",
        "\n",
        "        detected_map = apply_canny(img, low_threshold, high_threshold)\n",
        "        detected_map = HWC3(detected_map)\n",
        "\n",
        "        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n",
        "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
        "        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
        "\n",
        "        if seed == -1:\n",
        "            seed = random.randint(0, 65535)\n",
        "        seed_everything(seed)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n",
        "        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n",
        "        shape = (4, H // 8, W // 8)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=True)\n",
        "\n",
        "        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
        "        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n",
        "                                                     shape, cond, verbose=False, eta=eta,\n",
        "                                                     unconditional_guidance_scale=scale,\n",
        "                                                     unconditional_conditioning=un_cond)\n",
        "\n",
        "        if config.save_memory:\n",
        "            model.low_vram_shift(is_diffusing=False)\n",
        "\n",
        "        x_samples = model.decode_first_stage(samples)\n",
        "        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
        "\n",
        "        results = [x_samples[i] for i in range(num_samples)]\n",
        "        #show_image(results[0])\n",
        "        result_image = Image.fromarray(results[0])\n",
        "        result_image.save(\"output.jpg\")\n",
        "        return result_image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GI-ID9zdXIbH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Put Image URL here\n",
        "\n",
        "%cd /content\n",
        "from PIL import Image\n",
        "import requests\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "from urllib.request import urlretrieve\n",
        "import PIL.Image\n",
        "import torchvision.transforms as T\n",
        "import fastai\n",
        "from fastai.vision import *\n",
        "from fastai.utils.mem import *\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "\n",
        "class FeatureLoss(nn.Module):\n",
        "    def __init__(self, m_feat, layer_ids, layer_wgts):\n",
        "        super().__init__()\n",
        "        self.m_feat = m_feat\n",
        "        self.loss_features = [self.m_feat[i] for i in layer_ids]\n",
        "        self.hooks = hook_outputs(self.loss_features, detach=False)\n",
        "        self.wgts = layer_wgts\n",
        "        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n",
        "              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n",
        " \n",
        "    def make_features(self, x, clone=False):\n",
        "        self.m_feat(x)\n",
        "        return [(o.clone() if clone else o) for o in self.hooks.stored]\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        out_feat = self.make_features(target, clone=True)\n",
        "        in_feat = self.make_features(input)\n",
        "        self.feat_losses = [base_loss(input,target)]\n",
        "        self.feat_losses += [base_loss(f_in, f_out)*w\n",
        "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
        "        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n",
        "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
        "        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n",
        "        return sum(self.feat_losses)\n",
        "    \n",
        "    def __del__(self): self.hooks.remove()\n",
        "\n",
        "MODEL_URL = \"https://www.dropbox.com/s/starqc9qd2e1lg1/ArtLine_650.pkl?dl=1\"\n",
        "#download if not already downloaded\n",
        "if not os.path.exists(\"ArtLine_650.pkl\"):\n",
        "    urllib.request.urlretrieve(MODEL_URL, \"ArtLine_650.pkl\")\n",
        "path = Path(\".\")\n",
        "learn=load_learner(path, 'ArtLine_650.pkl')\n",
        "\n",
        "from PIL import Image,ImageOps\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import fastai\n",
        "from fastai.vision import *\n",
        "from fastai.utils.mem import *\n",
        "from fastai.vision import open_image, load_learner, image, torch\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import PIL.Image\n",
        "from io import BytesIO\n",
        "import torchvision.transforms as T\n",
        "def predict(url):\n",
        "  response = requests.get(url)\n",
        "  input = PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "  size = input.size\n",
        "  img_t = T.ToTensor()(input)\n",
        "  img_fast = Image(img_t)\n",
        "  p,img_hr,b = learn.predict(img_fast)\n",
        "  x = np.minimum(np.maximum(image2np(img_hr.data*255), 0), 255).astype(np.uint8)\n",
        "  img = PIL.Image.fromarray(x)\n",
        "  #im = img.resize(size)\n",
        "  img.save(\"test.png\")\n",
        "\n",
        "\n",
        "url = 'https://pbs.twimg.com/media/FBhPpw8XIAMWFpN.jpg' #@param {type:\"string\"}\n",
        "predict(url)"
      ],
      "metadata": {
        "id": "c5DBxUSlWeVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02327a9-1fe1-401b-936c-c706e1f0cbf6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Prompt\n",
        "\n",
        "prompt = \"wolp artgerm\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "QIj8T556d82p",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1o_mEchLltqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title ## Show\n",
        "from PIL import Image\n",
        "\n",
        "import cv2\n",
        "\n",
        "# Load the image\n",
        "input_image = cv2.imread('/content/test.png')\n",
        "# Define the default arguments\n",
        "a_prompt = \"best quality, extremely detailed\"\n",
        "n_prompt = \"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality\"\n",
        "num_samples = 1\n",
        "image_resolution = 512\n",
        "ddim_steps = 20\n",
        "guess_mode = False\n",
        "strength = 1.0\n",
        "scale = 6.0\n",
        "seed = -1\n",
        "eta = 0.0\n",
        "low_threshold = 50\n",
        "high_threshold = 60\n",
        "\n",
        "# Call the process function with the arguments\n",
        "process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, guess_mode, strength, scale, seed, eta, low_threshold, high_threshold)\n"
      ],
      "metadata": {
        "id": "NXnQEhV6dbtc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}